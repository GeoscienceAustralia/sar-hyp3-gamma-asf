{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using the HyP3 SDK for Python\n",
    "https://nbviewer.org/github/ASFHyP3/hyp3-sdk/blob/main/docs/sdk_example.ipynb#Submitting-Sentinel-1-InSAR-jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asf_search as asf\n",
    "import hyp3_sdk as sdk\n",
    "from hyp3_sdk import HyP3\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy\n",
    "import geopandas as gpd\n",
    "import pyproj\n",
    "import antimeridian\n",
    "import os\n",
    "import zipfile\n",
    "import rasterio\n",
    "from utils import upload_file\n",
    "import json\n",
    "import ntpath \n",
    "import shutil\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_from_bounds(xmin, xmax, ymin, ymax):\n",
    "    point1 = (float(xmin), float(ymax))\n",
    "    point2 = (float(xmax), float(ymax))\n",
    "    point3 = (float(xmax), float(ymin))\n",
    "    point4 = (float(xmin), float(ymin))\n",
    "    # Create a Shapely polygon from the points\n",
    "    aoi = Polygon([point1, point2, point3, point4])\n",
    "    return aoi\n",
    "\n",
    "def transform_polygon(src_crs, dst_crs, geometry, always_xy=True):\n",
    "    src_crs = pyproj.CRS(f\"EPSG:{src_crs}\")\n",
    "    dst_crs = pyproj.CRS(f\"EPSG:{dst_crs}\") \n",
    "    transformer = pyproj.Transformer.from_crs(src_crs, dst_crs, always_xy=always_xy)\n",
    "     # Transform the polygon's coordinates\n",
    "    transformed_exterior = [\n",
    "        transformer.transform(x, y) for x, y in geometry.exterior.coords\n",
    "    ]\n",
    "    # Create a new Shapely polygon with the transformed coordinates\n",
    "    transformed_polygon = Polygon(transformed_exterior)\n",
    "    return transformed_polygon\n",
    "\n",
    "def plot_polygons(polygons, bounds, crs=4326, colour='dodgerblue', colours=[], proj='stereo'):\n",
    "    plt.rcParams[\"figure.figsize\"] = [10,8]\n",
    "    # plot the the product geometries on a map\n",
    "    east, west, south, north = bounds\n",
    "    if proj=='stereo':\n",
    "        ax = plt.axes(projection=cartopy.crs.SouthPolarStereo())\n",
    "    else:\n",
    "        ax = plt.axes(projection=cartopy.crs.PlateCarree())\n",
    "    ax.set_extent((east, west, south, north), cartopy.crs.PlateCarree())\n",
    "    ax.add_feature(cartopy.feature.LAND)\n",
    "    ax.add_feature(cartopy.feature.OCEAN)\n",
    "    #proj = cartopy.crs.SouthPolarStereo() if crs==3031 else cartopy.crs.PlateCarree()\n",
    "    for i,polygon in enumerate(polygons):\n",
    "        c = colours[i] if colours else colour\n",
    "        ax.add_geometries(polygon, \n",
    "                        crs=cartopy.crs.PlateCarree(), \n",
    "                        alpha=0.3, \n",
    "                        edgecolor='black',\n",
    "                        facecolor=c,\n",
    "                        linewidth=1\n",
    "                        #fillcolor='white'\n",
    "                        ) \n",
    "    ax.gridlines(draw_labels=True)\n",
    "    ax.add_feature(cartopy.feature.COASTLINE)\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticating to the API\n",
    "\n",
    "The SDK will attempt to pull your [NASA Earthdata Login](https://urs.earthdata.nasa.gov/) credentials out of `~/.netrc`\n",
    "by default. Otherwise, you can add your credentials to the  `credentials/credentials_earthdata.yaml` file and the code below will read them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials/credentials_earthdata.yaml\", \"r\", encoding='utf8') as f:\n",
    "   earthdata_cfg = yaml.safe_load(f.read())\n",
    "   uid = earthdata_cfg['login']\n",
    "   pswd = earthdata_cfg['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp3 = HyP3(username=uid, password=pswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Search\n",
    "- There are two main searching methods implemented in this notebook. If the scenes of interest are known, a list can be specified. If the scenes are not known, a search can be made using the asf.search function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Search with list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if search_list:\n",
    "\n",
    "    search_scenes = [\n",
    "        'S1A_IW_SLC__1SDV_20240719T195219_20240719T195246_054834_06AD81_2850',\n",
    "        'S1A_IW_SLC__1SDV_20240625T195220_20240625T195247_054484_06A15E_25DD',\n",
    "        'S1A_IW_SLC__1SDV_20240601T195221_20240601T195248_054134_06953D_BA35',\n",
    "        'S1A_IW_SLC__1SDV_20240508T195222_20240508T195249_053784_06892A_14BF',\n",
    "        'S1A_IW_SLC__1SDV_20240414T195221_20240414T195248_053434_067B76_7297',\n",
    "        'S1A_IW_SLC__1SDV_20240321T195221_20240321T195248_053084_066DEF_BB5C',\n",
    "        'S1A_IW_SLC__1SDV_20240226T195220_20240226T195247_052734_066181_BA46',\n",
    "        'S1A_IW_SLC__1SDV_20240202T195221_20240202T195248_052384_0655AE_65A0',\n",
    "        'S1A_IW_SLC__1SDV_20240109T195221_20240109T195248_052034_0649D6_1FF6',\n",
    "        'S1A_IW_SLC__1SDV_20231216T195223_20231216T195250_051684_063DCD_4ADC',\n",
    "        'S1A_IW_SLC__1SDV_20231122T195224_20231122T195251_051334_0631B4_AC2B',\n",
    "        'S1A_IW_SLC__1SDV_20231029T195225_20231029T195251_050984_06259C_6024',\n",
    "        'S1A_IW_SLC__1SDV_20231005T195224_20231005T195251_050634_0619A2_216F',\n",
    "        'S1A_IW_SLC__1SDV_20230911T195224_20230911T195251_050284_060DB2_CC96',\n",
    "        'S1A_IW_SLC__1SDV_20230818T195223_20230818T195250_049934_0601BA_AE0C'\n",
    "    ]\n",
    "\n",
    "    # search the scene list with the specified product type\n",
    "    results = asf.granule_search(search_scenes, asf.ASFSearchOptions(processingLevel='SLC'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search for scenes using asf.search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searh through time\n",
    "if not search_list:\n",
    "\n",
    "    wkt = \"POINT (145.225097 -16.422876)\" # Daintree Rainforest\n",
    "\n",
    "    # product\n",
    "    # prod = 'GRD_HS' # IW\n",
    "    # prod = 'GRD_MD' # EW\n",
    "    prod = 'SLC'\n",
    "\n",
    "    # acquisition mode\n",
    "    mode = 'IW'\n",
    "    #mode = 'EW'\n",
    "    #mode = ['S1','S2','S3','S4','S5','S6']\n",
    "\n",
    "    start_date = '2023-08-01T00:00:00Z'\n",
    "    end_date = '2024-07-31T00:00:00Z'\n",
    "\n",
    "    results = asf.search(platform=[asf.PLATFORM.SENTINEL1], \n",
    "                        intersectsWith=wkt, \n",
    "                        maxResults=1000, \n",
    "                        processingLevel=prod,\n",
    "                        beamMode=mode,\n",
    "                        start=start_date,\n",
    "                        end=end_date,\n",
    "                        )\n",
    "    search_scenes = []\n",
    "    for r in reversed(results):\n",
    "        id_ = r.__dict__['umm']['GranuleUR'].split('-')[0]\n",
    "        search_scenes.append(id_)\n",
    "    \n",
    "    print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r in enumerate(results):\n",
    "     if (i%2==1):\n",
    "          continue\n",
    "     print(r.properties['sceneName'], r.properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below will pull the location information for each scene\n",
    "# This data can be saved to json file, or is plotted below\n",
    "scene_locations = {}\n",
    "scene_locations_3031 = {}\n",
    "for r in reversed(results):\n",
    "    id = r.__dict__['umm']['GranuleUR'].split('-')[0]\n",
    "    print(r.__dict__['umm']['GranuleUR'].split('-')[0])\n",
    "    print(r.__dict__['umm']['AdditionalAttributes'][1]['Values'],\n",
    "          r.__dict__['umm']['AdditionalAttributes'][17],\n",
    "          r.__dict__['umm']['AdditionalAttributes'][18],\n",
    "          r.__dict__['umm']['AdditionalAttributes'][29],\n",
    "          r.__dict__['umm']['OrbitCalculatedSpatialDomains'][0]) #[1] #['FRAME_NUMBER']\n",
    "    print(r.__dict__['umm']['SpatialExtent'])\n",
    "    points = (r.__dict__['umm']['SpatialExtent']['HorizontalSpatialDomain']\n",
    "              ['Geometry']['GPolygons'][0]['Boundary']['Points'])\n",
    "    points = [(p['Longitude'],p['Latitude']) for p in points]\n",
    "    poly = Polygon(points)\n",
    "    poly_3031 = transform_polygon(4326, 3031, poly, always_xy=True)\n",
    "    poly = antimeridian.fix_polygon(poly)\n",
    "    scene_locations[id] = poly\n",
    "    scene_locations_3031[id] = poly_3031\n",
    "\n",
    "# save locations to file\n",
    "save = False\n",
    "if save: \n",
    "    df = pd.DataFrame.from_dict(scene_locations, orient='index')\n",
    "    df = df.rename(columns = {0:'geometry'})\n",
    "    gdf = gpd.GeoDataFrame(df, crs=4326, geometry='geometry')\n",
    "    gdf.to_file('qld_timeseries_scene_locations.json')\n",
    "\n",
    "#plot\n",
    "colours = ['green' if 'IW' in x else ('blue' if 'EW' in x else 'red') for x in scene_locations.keys()]\n",
    "plot_polygons(scene_locations.values(), (-180,180,-90,-53), colours=colours)\n",
    "plot_polygons(scene_locations.values(), (110,160,-45,-8), colours=colours, proj='plate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downlod the scenes if required\n",
    "download = False\n",
    "download_folder = 'data'\n",
    "if download:\n",
    "    session = asf.ASFSession()\n",
    "    session.auth_with_creds(uid,pswd)\n",
    "    # download all results\n",
    "    scene_paths = []\n",
    "    for scene in results:\n",
    "        name = scene.__dict__['umm']['GranuleUR'].split('-')[0]\n",
    "        print(name)\n",
    "        path = os.path.join(download_folder, name)\n",
    "        scene.download(path=download_folder, session=session)\n",
    "        scene_paths.append(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting jobs\n",
    "\n",
    "The SDK provides a submit method for [all supported job types](https://hyp3-docs.asf.alaska.edu/products/).\n",
    "\n",
    "### Submitting Sentinel-1 RTC jobs\n",
    "\n",
    "Sentinel-1 Radiometric Terrain Correction (RTC) jobs are submitted using [ESA granule IDs](https://sentinel.esa.int/web/sentinel/user-guides/sentinel-1-sar/naming-conventions).\n",
    "The example granules below can be viewed  in [ASF Search here](https://search.asf.alaska.edu/#/?zoom=7.08772014623877&center=-141.733866,58.498008&resultsLoaded=true&granule=S1A_IW_SLC__1SDV_20210214T154835_20210214T154901_036588_044C54_8494-SLC&searchType=List%20Search&searchList=S1A_IW_SLC__1SDV_20210214T154835_20210214T154901_036588_044C54_8494-SLC,S1B_IW_SLC__1SDV_20210210T153131_20210210T153159_025546_030B48_B568-SLC,S1A_IW_SLC__1SDV_20210210T025526_20210210T025553_036522_0449E2_7769-SLC,S1A_IW_SLC__1SDV_20210210T025501_20210210T025528_036522_0449E2_3917-SLC,S1B_IW_SLC__1SDV_20210209T030255_20210209T030323_025524_030A8D_7E88-SLC,S1B_IW_SLC__1SDV_20210209T030227_20210209T030257_025524_030A8D_5BAF-SLC,S1A_IW_SLC__1SDV_20210202T154835_20210202T154902_036413_044634_01A1-SLC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the proj name\n",
    "# a unique project name is needed \n",
    "PROJECT_NAME = 'daintree-rainforest-timeseries-20m-2024'\n",
    "SCENE_LIST = search_scenes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit jobs\n",
    "rtc_jobs = sdk.Batch()\n",
    "for g in SCENE_LIST:\n",
    "    rtc_jobs += hyp3.submit_rtc_job(g, \n",
    "                                    include_dem=True, #include dem in final product\n",
    "                                    include_inc_map=True,  #include dem map in final product\n",
    "                                    include_rgb=True,  #include rgb img in final product\n",
    "                                    include_scattering_area=True, #include scat area in final product\n",
    "                                    name=PROJECT_NAME,\n",
    "                                    resolution=20,\n",
    "                                    dem_name='copernicus',\n",
    "                                    radiometry='gamma0')\n",
    "print(rtc_jobs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've given each job the name `rtc-example`, which we can use later to search for these jobs.\n",
    "\n",
    "`HyP3.submit_rtc_job` also accepts\n",
    "[keyword arguments](https://hyp3-docs.asf.alaska.edu/using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_rtc_job)\n",
    "to customize the RTC products to your application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtc_jobs = hyp3.find_jobs(name=PROJECT_NAME)\n",
    "rtc_jobs = hyp3.watch(rtc_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View which jobs have been completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PROJECT_NAME)\n",
    "rtc_jobs = hyp3.find_jobs(name=PROJECT_NAME)\n",
    "rtc_jobs = hyp3.refresh(rtc_jobs)\n",
    "running_jobs = rtc_jobs.filter_jobs(succeeded=False, running=True, failed=False)\n",
    "print(f'Number of running jobs: {len(running_jobs)}')\n",
    "succeeded_jobs = rtc_jobs.filter_jobs(succeeded=True, running=False, failed=False)\n",
    "print(f'Number of succeeded jobs: {len(succeeded_jobs)}')\n",
    "failed_jobs = rtc_jobs.filter_jobs(succeeded=False, running=False, failed=True)\n",
    "print(f'Number of failed jobs: {len(failed_jobs)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download, Unzip and Push to S3\n",
    "- The following blocks of code will download the scene from the asf to the local machine. The scene will then be unzipped and the files uploaded to the specified s3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_folder = '/data/hyp3-gamma'\n",
    "s3_bucket = 'deant-data-public-dev'\n",
    "s3_bucket_folder = f'sar-comparison-study/s1-rtc-scenes/{PROJECT_NAME}'\n",
    "push_to_s3 = True\n",
    "delete_after_upload = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set credentials for pushing to s3 bucket\n",
    "with open('credentials/credentials_aws.yaml', \"r\", encoding='utf8') as f:\n",
    "    aws_cfg = yaml.safe_load(f.read())\n",
    "    # set all keys as environment variables\n",
    "    for k in aws_cfg.keys():\n",
    "        os.environ[k] = aws_cfg[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_timing_dict():\n",
    "    # timing file with same structure as used for rtc-opera and\n",
    "    # pyrosar. Fill with zeros.\n",
    "    return {\n",
    "        \"Download Scene\":\t0,\n",
    "        \"Download Orbits\":\t0,\n",
    "        \"Download DEM\":\t0,\n",
    "        \"RTC Processing\":\t0,\n",
    "        \"S3 Upload\":\t0,\n",
    "        \"Delete Files\":\t0,\n",
    "        \"Total\":\t0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_jobs = succeeded_jobs\n",
    "for i,job in enumerate(download_jobs):\n",
    "    print(f'scene {i+1} of {len(download_jobs)}')\n",
    "    # download\n",
    "    file_list = job.download_files(download_folder)\n",
    "    process_time = job.processing_times\n",
    "    _, filename = ntpath.split(str(file_list[0]))\n",
    "    st = filename.split('_')[2]\n",
    "    scene = [x for x in SCENE_LIST if st in x][0]\n",
    "    print(scene)\n",
    "    # unzip\n",
    "    with zipfile.ZipFile(file_list[0], 'r') as zip_ref:\n",
    "        zip_ref.extractall(download_folder)\n",
    "    os.remove(file_list[0])\n",
    "    prod_folder = os.path.join(download_folder, str(file_list[0]).replace('.zip',''))\n",
    "    tif_file = [x for x in os.listdir(prod_folder) \n",
    "                    if (x.endswith('.tif') and any(ph in x for ph in ['HH','HV','VV','VH']))][0]\n",
    "    # get the crs from the tif\n",
    "    with rasterio.open(str(os.path.join(prod_folder, tif_file))) as src:\n",
    "        crs = src.meta['crs']\n",
    "        crs = str(crs).split(':')[-1]\n",
    "    #make the timing file\n",
    "    timing_dict = make_timing_dict()\n",
    "    timing_dict['Total'] = process_time\n",
    "    timing_dict['RTC Processing'] = process_time\n",
    "    timing_path = os.path.join(prod_folder,f'{scene}_timing.json')\n",
    "    with open(timing_path, 'w') as fp:\n",
    "        json.dump(timing_dict, fp)\n",
    "    for f in os.listdir(prod_folder):\n",
    "        f_path = os.path.join(prod_folder, f)\n",
    "        if push_to_s3:\n",
    "            bucket_folder = f\"{s3_bucket_folder}/hyp3-gamma/glo_30/{crs}/{scene}\"\n",
    "            object_name = f\"{bucket_folder}/{f}\"\n",
    "            upload_file(f_path,s3_bucket,object_name)\n",
    "    if delete_after_upload:\n",
    "        shutil.rmtree(prod_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Submitting Sentinel-1 InSAR jobs\n",
    "\n",
    "The SDK can also submit Sentinel-1 Interferometric Synthetic Aperture Radar (InSAR) jobs which processes\n",
    "reference and secondary granule pairs.\n",
    "\n",
    "For a particular reference granule, we may want to use the nearest and next-nearest temporal neighbor granules as secondary\n",
    "scenes. To programmatically find our secondary granules for a reference granule, We'll define a `get_nearest_neighbors`\n",
    "function that uses the [baseline stack](https://docs.asf.alaska.edu/asf_search/ASFProduct/#stack) method from `asf_search`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def get_nearest_neighbors(granule: str, max_neighbors: Optional[int] = None) -> asf.ASFSearchResults:\n",
    "    #granule = asf.granule_search(granule)[-1] #original code commented out because didnt work...\n",
    "    granule = asf.granule_search(granule)[0]\n",
    "    stack = reversed([item for item in granule.stack() if item.properties['temporalBaseline'] < 0])\n",
    "    results = asf.ASFSearchResults(stack)[:max_neighbors]\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, using the example granule list for our RTC jobs as the reference scenes, we can find their nearest and next-nearest neighbor granules, and submit them\n",
    "as pairs for InSAR processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # For a nice progress bar: https://github.com/tqdm/tqdm#ipython-jupyter-integration\n",
    "\n",
    "granules = [\n",
    "    'S1A_IW_SLC__1SDV_20210214T154835_20210214T154901_036588_044C54_8494',\n",
    "    'S1B_IW_SLC__1SDV_20210210T153131_20210210T153159_025546_030B48_B568',\n",
    "    'S1A_IW_SLC__1SDV_20210210T025526_20210210T025553_036522_0449E2_7769',\n",
    "    'S1A_IW_SLC__1SDV_20210210T025501_20210210T025528_036522_0449E2_3917',\n",
    "    'S1B_IW_SLC__1SDV_20210209T030255_20210209T030323_025524_030A8D_7E88',\n",
    "    'S1B_IW_SLC__1SDV_20210209T030227_20210209T030257_025524_030A8D_5BAF',\n",
    "    'S1A_IW_SLC__1SDV_20210202T154835_20210202T154902_036413_044634_01A1',\n",
    "]\n",
    "\n",
    "insar_jobs = sdk.Batch()\n",
    "for reference in tqdm(granules):\n",
    "    neighbors = get_nearest_neighbors(reference, max_neighbors=2)\n",
    "    for secondary in neighbors:\n",
    "        print(f'Reference: {reference}')\n",
    "        print(f'Secondary: {secondary.properties[\"sceneName\"]}')\n",
    "        insar_jobs += hyp3.submit_insar_job(\n",
    "            reference, \n",
    "            secondary.properties['sceneName'], \n",
    "            name='insar-example-extra',\n",
    "            include_look_vectors = True,\n",
    "            include_los_displacement = True,\n",
    "            include_inc_map = True,\n",
    "            looks = '20x4', #'20x4', '10x2'\n",
    "            include_dem = True,\n",
    "            include_wrapped_phase = True,\n",
    "            apply_water_mask = True,\n",
    "            include_displacement_maps = True\n",
    "            )\n",
    "print(insar_jobs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_jobs = hyp3.find_jobs(name='insar-example-extra')\n",
    "insar_jobs = hyp3.watch(insar_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_jobs = hyp3.refresh(insar_jobs)\n",
    "succeeded_jobs = insar_jobs.filter_jobs(succeeded=True, running=False, failed=False)\n",
    "print(f'Number of succeeded jobs: {len(succeeded_jobs)}')\n",
    "failed_jobs = insar_jobs.filter_jobs(succeeded=False, running=False, failed=True)\n",
    "print(f'Number of failed jobs: {len(failed_jobs)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = succeeded_jobs.download_files()\n",
    "file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check status"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Like RTC jobs, `HyP3.submit_insar_job` accepts\n",
    "[keyword arguments](https://hyp3-docs.asf.alaska.edu/using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_insar_job)\n",
    "to customize the InSAR products to your application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting autoRIFT jobs\n",
    "\n",
    "AutoRIFT supports processing Sentinel-1, Sentinel-2, or Landsat-8 Collection 2 pairs.\n",
    "* Sentinel-1 jobs are submitted using [ESA granule IDs](https://sentinel.esa.int/web/sentinel/user-guides/sentinel-1-sar/naming-conventions)\n",
    "* Sentinel-2 jobs are submitted using [ESA granule IDs](https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/naming-convention)\n",
    "* Landsat-8 Collection 2 jobs are submitted using [USGS scene IDs](https://www.usgs.gov/faqs/what-naming-convention-landsat-collection-2-level-1-and-level-2-scenes?qt-news_science_products=0#qt-news_science_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autorift_pairs = [\n",
    "    # Sentinel-1 ESA granule IDs\n",
    "    ('S1A_IW_SLC__1SSH_20170221T204710_20170221T204737_015387_0193F6_AB07',\n",
    "     'S1B_IW_SLC__1SSH_20170227T204628_20170227T204655_004491_007D11_6654'),\n",
    "    # Sentinel-2 ESA granule IDs\n",
    "    ('S2B_MSIL1C_20200612T150759_N0209_R025_T22WEB_20200612T184700',\n",
    "     'S2A_MSIL1C_20200627T150921_N0209_R025_T22WEB_20200627T170912'),\n",
    "    # Landsat 8\n",
    "    ('LC08_L1TP_009011_20200703_20200913_02_T1',\n",
    "     'LC08_L1TP_009011_20200820_20200905_02_T1'),\n",
    "]\n",
    "\n",
    "autorift_jobs = sdk.Batch()\n",
    "for reference, secondary in autorift_pairs:\n",
    "    autorift_jobs += hyp3.submit_autorift_job(reference, secondary, name='autorift-example')\n",
    "print(autorift_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autorift_jobs = hyp3.watch(autorift_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autorift_jobs = hyp3.refresh(autorift_jobs)\n",
    "succeeded_jobs = autorift_jobs.filter_jobs(succeeded=True, running=False, failed=False)\n",
    "print(f'Number of succeeded jobs: {len(succeeded_jobs)}')\n",
    "failed_jobs = autorift_jobs.filter_jobs(succeeded=False, running=False, failed=True)\n",
    "print(f'Number of failed jobs: {len(failed_jobs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = succeeded_jobs.download_files()\n",
    "file_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
